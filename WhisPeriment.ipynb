{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisPeriment\n",
    "Experimenting with the Whisper model\n",
    "- Record an audio sequence\n",
    "- Allow to replay it\n",
    "- Provide the text in Engilsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create --name hugg python=3.10 -y\n",
    "# conda activate hugg \n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia -y\n",
    "# conda install transformers diffusers[\"torch\"] pyaudio -y\n",
    "# conda install numpy scipy ipywidgets ipykernel -y\n",
    "# conda install accelerate -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyaudio      : 0.2.12\n",
      "wave         : Nov version infos available\n",
      "numpy        : 1.24.0\n",
      "transformers : 4.24.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import struct\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy           import signal\n",
    "from transformers    import WhisperProcessor, WhisperForConditionalGeneration, __version__ \n",
    "from transformers    import pipeline, WhisperTokenizer, WhisperModel, WhisperFeatureExtractor, __version__ \n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(f\"pyaudio      : {      pyaudio.__version__}\")\n",
    "print(f\"wave         : Nov version infos available\")\n",
    "print(f\"numpy        : {           np.__version__}\")\n",
    "print(f\"transformers : {             __version__ }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Whisper model\n",
    "Choose the model depending of execution time and Available GPU mem:\n",
    "- 12G -> large 'Note there has been a recent improvment of the large model)\n",
    "-  6G -> medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper = pipeline('automatic-speech-recognition', model = 'openai/whisper-medium', device =0)\n",
    "\n",
    "# instantiate the tokenizer and set the prefix token\n",
    "#tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"French\")\n",
    "\n",
    "# now switch the prefix token from Spanish to French\n",
    "#tokenizer.set_prefix_tokens(language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_normalize   = (1.0/32768.0)\n",
    "chunk             = 1024\n",
    "swidth            = 2 \n",
    "frames_per_buffer = 12800\n",
    "format            = pyaudio.paInt16\n",
    "channels          = 1\n",
    "sample_rate       = 48000 #44100\n",
    "target_rate       = 16000\n",
    "device_index      = 5\n",
    "audio_file        = 'record.wav'\n",
    "\n",
    "def rms(frame):\n",
    "  \"\"\"Return the RMS value of the frame content\"\"\"\n",
    "  count       = len(frame) / swidth\n",
    "  format      = \"%dh\" % (count)\n",
    "  shorts      = struct.unpack(format, frame)\n",
    "  sum_squares = 0.0\n",
    "  for sample in shorts:\n",
    "      n            = sample * short_normalize\n",
    "      sum_squares += n * n\n",
    "  rms = math.pow(sum_squares / count, 0.5)\n",
    "\n",
    "  return rms * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(audio, input_rate, output_rate):\n",
    "  \"\"\"\n",
    "  ALSA only support 44100 or 48000 sampling rate, resampleing from input_rate to output_rate \n",
    "  Args:\n",
    "      audio (binary)   : Input audio stream\n",
    "      input_rate (int) : Input audio rate to resample from\n",
    "      output_rate (int): Input audio rate to resample from   \n",
    "  Return:\n",
    "      a numpy array of int16 resampled at the proper sample rate\n",
    "  \"\"\"\n",
    "  audio_i16     = np.frombuffer(buffer=audio, dtype=np.int16)\n",
    "  resample_size = int(len(audio_i16) / input_rate * output_rate)\n",
    "  resample      = signal.resample(audio_i16, resample_size)\n",
    "  out_i16       = np.array(resample, dtype=np.int16)\n",
    "  #print(f\"input size: {len(audio_i16)}, output zize:{len(out_i16)}\")\n",
    "  \n",
    "  return out_i16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Now Recording for 30 s-----\n",
      "-----End Recording----- Last RMS:  37.72 \n"
     ]
    }
   ],
   "source": [
    "t_record_s  = 30\n",
    "n_chucnk    = int(t_record_s * sample_rate / chunk)\n",
    "p           = pyaudio.PyAudio()\n",
    "stream      = p.open(    \n",
    "   format   = format,  channels          = channels,         rate = sample_rate,\n",
    "   input    = True,    frames_per_buffer = frames_per_buffer, input_device_index=device_index)\n",
    "   \n",
    "frames = []\n",
    "print(f\"-----Now Recording for {t_record_s} s-----\")\n",
    "for i in range(0,400):\n",
    "  audio_data   = stream.read(chunk, exception_on_overflow = False)\n",
    "  rms_val      = rms(audio_data)\n",
    "  frames.append(resample(audio_data, sample_rate, target_rate))\n",
    "\n",
    "print(f'-----End Recording----- Last RMS: {rms_val:6.2f} ')\n",
    "stream.stop_stream()    # Stop Audio Recording  IMPORTANT\n",
    "stream.close()          # Close Audio Recording IMPORTANT\n",
    "#print(frames)\n",
    "\n",
    "\n",
    "wf = wave.open(audio_file, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(format))\n",
    "wf.setframerate(target_rate)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.stop_stream()    # Stop Audio Recording  IMPORTANT\n",
    "stream.close()          # Close Audio Recording IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(Audio(audio_file, autoplay=True, rate=target_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/me/miniforge3/envs/hugg/lib/python3.10/site-packages/transformers/generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' The work that makes life better. Solar, the power of comfort. And the antipathetic antimajor. Solar. Solar.'}\n"
     ]
    }
   ],
   "source": [
    "text = whisper(audio_file, sample_rate=target_rate)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
    "selection = widgets.Dropdown(\n",
    "    options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n",
    "    value=\"ko_kr\",\n",
    "    description='Language:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lang = selection.value\n",
    "language = languages[lang]\n",
    "\n",
    "assert lang is not None, \"Please select a language\"\n",
    "print(f\"Selected language: {language} ({lang})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whis      = WhisperProcessor.from_pretrained('openai/whisper-medium', language=\"French\")\n",
    "#tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"French\")\n",
    "\n",
    "processor         = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model             = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "#model             = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"fr\", task = \"transcribe\")\n",
    "configuration     = model.config\n",
    "#print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_speech = frames\n",
    "input_features = processor(input_speech, ,return_tensors=\"pt\").input_features \n",
    "predicted_ids  = model.generate(input_features)\n",
    "transcription  = processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ge\n",
    "\n",
    "audio = WhisperModel. .load_audio(audio_file)\n",
    "audio = WhisperModel.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = whis(frames, sampling_rate=target_rate)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tokenizer and set the prefix token\n",
    "#tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"French\")\n",
    "\n",
    "# now switch the prefix token from Spanish to French\n",
    "#tokenizer.set_prefix_tokens(language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifu the audio input devices\n",
    "import pyaudio\n",
    "p          = pyaudio.PyAudio()\n",
    "info       = p.get_host_api_info_by_index(0)\n",
    "numdevices = info.get('deviceCount')\n",
    "for i in range(0, numdevices):\n",
    "        if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "            print( \"Input Device id \", i, \" - \", p.get_device_info_by_host_api_device_index(0, i).get('name'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dfc79af71ad0502366e9b84d0f3ceca25396bd089c65695a63dd9f59e99fd78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
