{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhisPeriment\n",
    "Experimenting with the Whisper model\n",
    "- Record an audio sequence in the defined language\n",
    "- Allow to replay it\n",
    "- Provide the text in the defined language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to execute to have the right environement to run this\n",
    "\"\"\"\n",
    "conda env remove -n hugg\n",
    "conda create --name hugg python=3.10 -y\n",
    "conda activate hugg\n",
    "pip install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "pip install -U transformers[torch]\n",
    "pip install -U numpy scipy ipywidgets ipykernel \n",
    "pip install -U accelerate \n",
    "pip install -U sentencepiece\n",
    "pip install -U pvrecorder -q\n",
    "pip install -U ffmpeg\n",
    "pip install -U webrtcvad\n",
    "pip install -U pyaudio\n",
    "\"\"\"\n",
    "\n",
    "# ffmpeg need to be installed on your computer with \n",
    "\"\"\"\n",
    "sudo apt install ffmpeg\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import wave\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io        import wavfile\n",
    "from pvrecorder      import PvRecorder\n",
    "from transformers    import WhisperProcessor, WhisperForConditionalGeneration, __version__ \n",
    "#from transformers    import pipeline, WhisperTokenizer, WhisperModel, WhisperFeatureExtractor, __version__ \n",
    "from IPython.display import Audio, display\n",
    "\n",
    "#print(f\"pyaudio      : {      pyaudio.__version__}\")\n",
    "print(f\"wave         : Nov version infos available\")\n",
    "print(f\"numpy        : {           np.__version__}\")\n",
    "print(f\"transformers : {             __version__ }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Whisper model\n",
    "Choose the model depending of execution time and Available GPU mem:\n",
    "- 12G -> large 'thanks to the large-v2 model there has been a improvments of the large model)\n",
    "-  6G -> medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all audio input devices \n",
    "\n",
    "for index, device in enumerate(PvRecorder.get_audio_devices()):\n",
    "    print(f\"[{index}] {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_index      = 1\n",
    "t_record_s        = 15\n",
    "\n",
    "channels          = 1\n",
    "target_rate       = 16000\n",
    "frame_length      = 512\n",
    "#language          = 'fr'\n",
    "language          = 'en'\n",
    "audio_file        = 'record.wav'\n",
    "#model_name        = \"openai/whisper-large-v2\"\n",
    "model_name        = \"openai/whisper-medium\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model     = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = language, task = \"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = int(1+t_record_s * target_rate / frame_length)\n",
    "recorder = PvRecorder(device_index=device_index, frame_length=frame_length) #(32 milliseconds of 16 kHz audio)\n",
    "audio    = []\n",
    "\n",
    "recorder.start()\n",
    "for i in range(0,n_frames):\n",
    "    frame = recorder.read()\n",
    "    audio.extend(frame)\n",
    "\n",
    "recorder.stop()\n",
    "with wave.open(audio_file, 'w') as f:\n",
    "    f.setparams((1, 2, target_rate, frame_length, \"NONE\", \"NONE\"))\n",
    "    f.writeframes(struct.pack(\"h\" * len(audio), *audio))\n",
    "\n",
    "recorder.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(audio_file, autoplay=True, rate=target_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_rate, input_speech = wavfile.read(audio_file)\n",
    "print(f\"Audio file recorded at: {sample_rate} samples per seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = processor(input_speech, return_tensors=\"pt\", sampling_rate=sample_rate).input_features \n",
    "predicted_ids  = model.generate(input_features, max_new_tokens=100)\n",
    "transcription  = processor.batch_decode(predicted_ids, skip_special_tokens = True)\n",
    "print(transcription)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMPSTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = whisper(audio_file, sample_rate=target_rate, framework=\"pt\", max_new_tokens=100, max_length=100)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "languages = {\"af_za\": \"Afrikaans\", \"am_et\": \"Amharic\", \"ar_eg\": \"Arabic\", \"as_in\": \"Assamese\", \"az_az\": \"Azerbaijani\", \"be_by\": \"Belarusian\", \"bg_bg\": \"Bulgarian\", \"bn_in\": \"Bengali\", \"bs_ba\": \"Bosnian\", \"ca_es\": \"Catalan\", \"cmn_hans_cn\": \"Chinese\", \"cs_cz\": \"Czech\", \"cy_gb\": \"Welsh\", \"da_dk\": \"Danish\", \"de_de\": \"German\", \"el_gr\": \"Greek\", \"en_us\": \"English\", \"es_419\": \"Spanish\", \"et_ee\": \"Estonian\", \"fa_ir\": \"Persian\", \"fi_fi\": \"Finnish\", \"fil_ph\": \"Tagalog\", \"fr_fr\": \"French\", \"gl_es\": \"Galician\", \"gu_in\": \"Gujarati\", \"ha_ng\": \"Hausa\", \"he_il\": \"Hebrew\", \"hi_in\": \"Hindi\", \"hr_hr\": \"Croatian\", \"hu_hu\": \"Hungarian\", \"hy_am\": \"Armenian\", \"id_id\": \"Indonesian\", \"is_is\": \"Icelandic\", \"it_it\": \"Italian\", \"ja_jp\": \"Japanese\", \"jv_id\": \"Javanese\", \"ka_ge\": \"Georgian\", \"kk_kz\": \"Kazakh\", \"km_kh\": \"Khmer\", \"kn_in\": \"Kannada\", \"ko_kr\": \"Korean\", \"lb_lu\": \"Luxembourgish\", \"ln_cd\": \"Lingala\", \"lo_la\": \"Lao\", \"lt_lt\": \"Lithuanian\", \"lv_lv\": \"Latvian\", \"mi_nz\": \"Maori\", \"mk_mk\": \"Macedonian\", \"ml_in\": \"Malayalam\", \"mn_mn\": \"Mongolian\", \"mr_in\": \"Marathi\", \"ms_my\": \"Malay\", \"mt_mt\": \"Maltese\", \"my_mm\": \"Myanmar\", \"nb_no\": \"Norwegian\", \"ne_np\": \"Nepali\", \"nl_nl\": \"Dutch\", \"oc_fr\": \"Occitan\", \"pa_in\": \"Punjabi\", \"pl_pl\": \"Polish\", \"ps_af\": \"Pashto\", \"pt_br\": \"Portuguese\", \"ro_ro\": \"Romanian\", \"ru_ru\": \"Russian\", \"sd_in\": \"Sindhi\", \"sk_sk\": \"Slovak\", \"sl_si\": \"Slovenian\", \"sn_zw\": \"Shona\", \"so_so\": \"Somali\", \"sr_rs\": \"Serbian\", \"sv_se\": \"Swedish\", \"sw_ke\": \"Swahili\", \"ta_in\": \"Tamil\", \"te_in\": \"Telugu\", \"tg_tj\": \"Tajik\", \"th_th\": \"Thai\", \"tr_tr\": \"Turkish\", \"uk_ua\": \"Ukrainian\", \"ur_pk\": \"Urdu\", \"uz_uz\": \"Uzbek\", \"vi_vn\": \"Vietnamese\", \"yo_ng\": \"Yoruba\"}\n",
    "selection = widgets.Dropdown(\n",
    "    options=[(\"Select language\", None), (\"----------\", None)] + sorted([(f\"{v} ({k})\", k) for k, v in languages.items()]),\n",
    "    value=\"ko_kr\",\n",
    "    description='Language:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lang = selection.value\n",
    "language = languages[lang]\n",
    "\n",
    "assert lang is not None, \"Please select a language\"\n",
    "print(f\"Selected language: {language} ({lang})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whis      = WhisperProcessor.from_pretrained('openai/whisper-medium', language=\"French\")\n",
    "#tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"French\")\n",
    "\n",
    "processor         = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "model             = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "#model             = WhisperModel.from_pretrained(\"openai/whisper-base\")\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language = \"fr\", task = \"transcribe\")\n",
    "configuration     = model.config\n",
    "#print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_speech   = frames\n",
    "input_features = processor(input_speech, ,return_tensors=\"pt\").input_features \n",
    "predicted_ids  = model.generate(input_features)\n",
    "transcription  = processor.batch_decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ge\n",
    "\n",
    "audio = WhisperModel. .load_audio(audio_file)\n",
    "audio = WhisperModel.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = whis(frames, sampling_rate=target_rate)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the tokenizer and set the prefix token\n",
    "#tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"French\")\n",
    "\n",
    "# now switch the prefix token from Spanish to French\n",
    "#tokenizer.set_prefix_tokens(language=\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifu the audio input devices\n",
    "import pyaudio\n",
    "p          = pyaudio.PyAudio()\n",
    "info       = p.get_host_api_info_by_index(0)\n",
    "numdevices = info.get('deviceCount')\n",
    "for i in range(0, numdevices):\n",
    "        if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "            print( \"Input Device id \", i, \" - \", p.get_device_info_by_host_api_device_index(0, i).get('name'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS\n",
    "https://github.com/amrrs/pyaudiorec/blob/main/audio_rec.ipynb\n",
    "\n",
    "!pip3 install pvrecorder -q #install pvrecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load PvRecorder\n",
    "\n",
    "from pvrecorder import PvRecorder\n",
    "import wave, struct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all audio input devices \n",
    "\n",
    "for index, device in enumerate(PvRecorder.get_audio_devices()):\n",
    "    print(f\"[{index}] {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(frame):\n",
    "  \"\"\"Return the RMS value of the frame content\"\"\"\n",
    "  count       = len(frame) / swidth\n",
    "  format      = \"%dh\" % (count)\n",
    "  shorts      = struct.unpack(format, frame)\n",
    "  sum_squares = 0.0\n",
    "  for sample in shorts:\n",
    "      n            = sample * short_normalize\n",
    "      sum_squares += n * n\n",
    "  rms = math.pow(sum_squares / count, 0.5)\n",
    "\n",
    "  return rms * 1000\n",
    "\n",
    "def resample(audio, input_rate, output_rate):\n",
    "  \"\"\"\n",
    "  ALSA only support 44100 or 48000 sampling rate, resampleing from input_rate to output_rate \n",
    "  Args:\n",
    "      audio (binary)   : Input audio stream\n",
    "      input_rate (int) : Input audio rate to resample from\n",
    "      output_rate (int): Input audio rate to resample from   \n",
    "  Return:\n",
    "      a numpy array of int16 resampled at the proper sample rate\n",
    "  \"\"\"\n",
    "  audio_i16     = np.frombuffer(buffer=audio, dtype=np.int16)\n",
    "  resample_size = int(len(audio_i16) / input_rate * output_rate)\n",
    "  resample      = signal.resample(audio_i16, resample_size)\n",
    "  out_i16       = np.array(resample, dtype=np.int16)\n",
    "  #print(f\"input size: {len(audio_i16)}, output zize:{len(out_i16)}\")\n",
    "  \n",
    "  return out_i16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chucnk    = int(t_record_s * sample_rate / chunk)\n",
    "p           = pyaudio.PyAudio()\n",
    "stream      = p.open(    \n",
    "   format   = format,  channels          = channels,         rate = sample_rate,\n",
    "   input    = True,    frames_per_buffer = frames_per_buffer, input_device_index=device_index)\n",
    "   \n",
    "frames = []\n",
    "print(f\"-----Now Recording for {t_record_s} s-----\")\n",
    "for i in range(0,400):\n",
    "  audio_data   = stream.read(chunk, exception_on_overflow = False)\n",
    "  rms_val      = rms(audio_data)\n",
    "  frames.append(resample(audio_data, sample_rate, target_rate))\n",
    "\n",
    "print(f'-----End Recording----- Last RMS: {rms_val:6.2f} ')\n",
    "stream.stop_stream()    # Stop Audio Recording  IMPORTANT\n",
    "stream.close()          # Close Audio Recording IMPORTANT\n",
    "#print(frames)\n",
    "\n",
    "\n",
    "wf = wave.open(audio_file, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(format))\n",
    "wf.setframerate(target_rate)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "help(transformers.models.whisper.feature_extraction_whisper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate   = 48000\n",
    "frame_length  = 2048 #int(0.02 * 48000)\n",
    "print(frame_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p      = pyaudio.PyAudio()       # Set up audio stream from microphone\n",
    "stream = p.open(format=pyaudio.paInt16, channels=1, rate=sample_rate, input=True)\n",
    "vad    = webrtcvad.Vad()         # Set up Voice Activity Detector (VAD)\n",
    "\n",
    "while True:                      # Process audio data in a loop\n",
    "    data      = stream.read(frame_length, exception_on_overflow=False)\n",
    "    is_speech = vad.is_speech(data, sample_rate)\n",
    "    if is_speech:\n",
    "        print(\"Speech detected!\")\n",
    "\n",
    "# Close audio stream\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dfc79af71ad0502366e9b84d0f3ceca25396bd089c65695a63dd9f59e99fd78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
